{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkZFooDvPf/8934LtHBMD2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sandeep81299/Sandeep-Salunke-Scifor/blob/main/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1.What is the difference between a list and a tuple in Python?**"
      ],
      "metadata": {
        "id": "1YKd-Ysd4Gne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lists and tuples are both used to store collections of data in Python, but they have key differences:\n",
        "\n",
        "Mutability: Lists are mutable, meaning you can change their contents (add, remove, or modify elements). Tuples are immutable, so their contents cannot be changed once defined.\n",
        "\n",
        "Syntax: Lists are defined using square brackets, while tuples use parentheses.\n",
        "\n",
        "Performance: Due to their immutability, tuples are slightly faster than lists. If you have a collection that should not change, using a tuple is more efficient.\n",
        "\n",
        "Use Cases: Lists are used when you need a collection of items that can be modified, while tuples are typically used for data that should not change, such as coordinates or configuration settings."
      ],
      "metadata": {
        "id": "I3NSqV6Q4QZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2.How can you iterate through a list in Python?**"
      ],
      "metadata": {
        "id": "8mgU1xBU4VHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can iterate through a list in Python using various methods, including:\n",
        "\n",
        "For Loop: The simplest way is to use a for loop to iterate through the elements of the list one by one."
      ],
      "metadata": {
        "id": "FI6tVUyq4h66"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP7RojZ6336Z",
        "outputId": "5dfb3bef-95c9-4ac1-cff5-e259f3d2ba49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "my_list = [1, 2, 3, 4, 5]\n",
        "for item in my_list:\n",
        "    print(item)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "List Comprehension: This concise method allows you to create a new list or perform an operation on each element while iterating."
      ],
      "metadata": {
        "id": "PbjkNbhF4vq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_list = [1, 2, 3, 4, 5]\n",
        "squared_list = [x**2 for x in my_list]\n"
      ],
      "metadata": {
        "id": "ieZerMF84w7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3.How do you handle exceptions in Python?**"
      ],
      "metadata": {
        "id": "5hfjxCOo42ag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python uses a try...except block to handle exceptions. The basic structure is as follows:"
      ],
      "metadata": {
        "id": "PdpIHLIT5C6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Code that might raise an exception\n",
        "    pass  # Placeholder code\n",
        "except ExceptionType as e:\n",
        "    # Code to handle the exception\n",
        "    pass  # Placeholder code\n",
        "else:\n",
        "    # Code to execute if no exception is raised\n",
        "    pass  # Placeholder code\n",
        "finally:\n",
        "    # Code that always runs, whether there was an exception or not\n",
        "    pass  # Placeholder code\n"
      ],
      "metadata": {
        "id": "D-9SdkO05thS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4.What are list comprehensions in Python?**"
      ],
      "metadata": {
        "id": "7UmCGczb52kH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "List comprehensions are a concise way to create lists in Python. They provide a compact syntax for creating lists by applying an expression to each item in an iterable. Here's an example that creates a list of squares:"
      ],
      "metadata": {
        "id": "zhRXufFP6AK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "squares = [x**2 for x in range(1, 6)]\n",
        "print(squares)\n",
        "# Result: [1, 4, 9, 16, 25]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad-S4Ymo6BzW",
        "outputId": "e2a82bc8-554a-4d70-8f8e-a0f45e5f8ca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 9, 16, 25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5.What is the purpose of the if __name__ == \"__main__\" statement?**"
      ],
      "metadata": {
        "id": "faYeDRJ46PIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, the if __name__ == \"__main__\" statement is used to determine whether a script is being run as the main program or if it is being imported as a module into another script. It allows you to have code that only executes when the script is run directly and not when it's imported elsewhere. This is often used for script initialization and testing."
      ],
      "metadata": {
        "id": "y2RmOCWI6cIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6.What is the purpose of the with statement in Python?**"
      ],
      "metadata": {
        "id": "6Zt-Yeab6Fp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'with' statement is used for simplifying the management of resources, such as files, sockets, and database connections. It ensures that the resources are properly acquired and released. It automatically takes care of setup and teardown actions, making your code cleaner and more readable.\n",
        "\n"
      ],
      "metadata": {
        "id": "bbTRcf-e6zC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**7.What are the key features of Spark?**"
      ],
      "metadata": {
        "id": "n740wy4K676A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apache Spark is a powerful, open-source, distributed computing system for big data processing. Its key features include:\n",
        "\n",
        "Speed: Spark is designed for in-memory data processing, making it faster than traditional disk-based systems.\n",
        "\n",
        "Ease of Use: Provides high-level APIs in Python, Java, and Scala, making it accessible to a wide range of developers.\n",
        "\n",
        "Versatility: Supports batch processing, real-time streaming, machine learning, and graph processing.\n",
        "\n",
        "Fault Tolerance: Spark can recover lost data due to node failures."
      ],
      "metadata": {
        "id": "Bu0yVmpQ7B8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**8.What are Resilient Distributed Datasets (RDDs) in Spark?**"
      ],
      "metadata": {
        "id": "CRR6lcR37GQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDDs are fundamental data structures in Spark. They are distributed, fault-tolerant collections of data that can be processed in parallel. RDDs are immutable, and transformations are executed lazily. They offer fault tolerance, in-memory processing, and are the building blocks for Spark applications."
      ],
      "metadata": {
        "id": "Udx_GYXI7Lld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**9.What is the difference between a DataFrame and an RDD in Spark?**"
      ],
      "metadata": {
        "id": "qc0YKRJq7g4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataFrame: DataFrames are higher-level abstractions built on top of RDDs. They are distributed collections of data organized into named columns. DataFrames are designed for structured data and provide optimizations for querying and filtering.\n",
        "\n",
        "RDD (Resilient Distributed Dataset): RDDs are low-level, distributed data structures that represent a collection of objects. They are more flexible and can handle unstructured data but require more manual optimization."
      ],
      "metadata": {
        "id": "hB8YcKVU7c-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**10.What is Spark's ecosystem?**"
      ],
      "metadata": {
        "id": "nvAVC5ts7o61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Spark ecosystem consists of various components and libraries, including:\n",
        "\n",
        "Spark SQL: Allows SQL-like querying of structured data within Spark.\n",
        "\n",
        "Spark Streaming: Provides real-time data processing capabilities.\n",
        "\n",
        "MLlib: A machine learning library for Spark.\n",
        "\n",
        "GraphX: A graph processing library.\n",
        "\n",
        "SparkR: An R package for Spark.\n",
        "\n",
        "Cluster Managers: Supports integration with cluster managers like Apache Hadoop YARN and Apache Mesos.\n",
        "\n",
        "Third-party Libraries: Many third-party libraries and tools have been developed to work with Spark, enhancing its capabilities for big data processing.\n",
        "\n",
        "This comprehensive ecosystem makes Apache Spark a versatile and powerful tool for big data processing and analysis."
      ],
      "metadata": {
        "id": "KLJIHuED7-Cj"
      }
    }
  ]
}